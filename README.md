# Attention-is-all-you-need-
Rebuilding the "Attention Is All You Need" Transformer architecture from scratch. Full implementation of multi-head attention, positional encoding, encoder-decoder stack, and training pipeline using PyTorch/NumPy with clear explanations.
